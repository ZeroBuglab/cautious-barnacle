"""
–ü–∞—Ä—Å–µ—Ä –¥–∞–Ω–Ω—ã—Ö
–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è —Å–±–æ—Ä–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –≤–µ–±-—Å–∞–π—Ç–æ–≤ –∏–ª–∏ API.
üìò –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- –ü–æ–ª—É—á–µ–Ω–∏–µ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü —Å –ø–æ–º–æ—â—å—é requests
- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω—É–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—á–µ—Ä–µ–∑ BeautifulSoup)
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ CSV –∏–ª–∏ JSON
"""
# –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫
import requests
from bs4 import BeautifulSoup
import pandas as pd
import json

# –ü—Ä–∏–º–µ—Ä —à–∞–±–ª–æ–Ω–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–∞—Ä—Å–µ—Ä–∞
def parse_example():
    """–ü—Ä–∏–º–µ—Ä —Ñ—É–Ω–∫—Ü–∏–∏ –ø–∞—Ä—Å–µ—Ä–∞."""
    url = "https://example.com"  # —Å—é–¥–∞ –≤—Å—Ç–∞–≤—å –Ω—É–∂–Ω—ã–π —Å–∞–π—Ç
    response = requests.get(url)

    if response.status_code == 200:
        soup = BeautifulSoup(response.text, "html.parser")

        # –ø—Ä–∏–º–µ—Ä: –Ω–∞–π—Ç–∏ –≤—Å–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ h2
        titles = [t.text for t in soup.find_all("h2")]

        # –≤—ã–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print("–ù–∞–π–¥–µ–Ω–æ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤:", len(titles))
        for t in titles:
            print("-", t)

        # —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ JSON
        with open("data.json", "w", encoding="utf-8") as f:
            json.dump(titles, f, ensure_ascii=False, indent=4)
    else:
        print("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ:", response.status_code)


if name == "__main__":

    parse_example()
